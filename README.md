<h1 align="center">VLA-Research</h1>

<p align="center"> </p>


> VLA-Research includes papers and open-source projects related to VLA, especially in the field of embodied manipulation. It aims to collect and summarize all VLA work, hoping to help friends with embodied AI to understand VLA work efficiently. Welcome to click Star, share and communicate.
# ğŸ“ TODO - å¾…æ›´æ–°
| ä»»åŠ¡ | çŠ¶æ€ | æè¿° |
|------|------|------|
| ğŸ”µ VLAåˆ†ç±» | å¾…å®Œæˆ | å¯¹VLAæ¨¡å‹è¿›è¡Œåˆ†ç±»æ•´ç† |
| ğŸ’  VLA-RLæ•´ç† | å¾…å®Œæˆ | æ•´ç†VLAä¸å¼ºåŒ–å­¦ä¹ ç›¸å…³çš„å·¥ä½œ |

# Contents - ç›®å½•

<nav>
  <ul>
    <li><a href="#VLA Research List">VLA Research List</a></li>
    <li><a href="#acknowledgement">Acknowledgement</a></li>
    <li><a href="#cite">ğŸ‘ Citation</a></li>
    <li><a href="#license">ğŸ·ï¸ License</a></li>
  </ul>
</nav>



<section id="VLA Research List"></section>


### 1. è‡ªå›å½’æ¨¡å‹ (Autoregressive Models)

| æ¨¡å‹åç§° | Paper | Code | Project/Blog | æœºæ„ | æ—¶é—´ | æ¨¡å‹å¤§å° | ä¼šè®®/æœŸåˆŠ | ç‰¹ç‚¹ |
|---------|-------|------|--------------|------|------|----------|-----------|------|
| RT-1 | [paper](https://arxiv.org/abs/2212.06817) | - | - | Google Deepmind | 2022.12 | - | Science Robotics | æœºå™¨äººTransformerç³»åˆ—çš„å¼€å±±ä¹‹ä½œ |
| RT-2 | [paper](https://arxiv.org/abs/2307.15818) | - | [page](https://robotics-transformer2.github.io/) | Google Deepmind | 2023.7 | 55B | Science Robotics | æœºå™¨äººTransformerç¬¬äºŒä»£ï¼Œæ”¯æŒè§†è§‰æ¨ç† |
| RT-Trajectory | [paper](https://arxiv.org/pdf/2311.01977) | - | - | Google Deepmind, UCSD, æ–¯å¦ç¦ | 2023.11 | - | Science Robotics | è½¨è¿¹é¢„æµ‹èƒ½åŠ›å¢å¼º |
| AUTORT | [paper](https://arxiv.org/abs/2401.12963) | - | - | Google Deepmind | 2024.1 | - | Science Robotics | è‡ªåŠ¨åŒ–æœºå™¨äººTransformer |
| RoboFlamingo | [paper](https://arxiv.org/abs/2311.01378) | [code](https://github.com/roboflamingo) | - | å­—èŠ‚ã€æ¸…å | 2024.2 | - | ICLR 2024 | åŸºäºFlamingoæ¶æ„ |
| OpenVLA | [paper](https://arxiv.org/pdf/2406.09246) | [code](https://github.com/openvla) | - | Stanford | 2024.6 | 7B | RSS 2024 | å¼€æºVLAæ¨¡å‹ |
| TinyVLA | [paper](https://arxiv.org/abs/2409.12514) | - | - | ä¸Šæµ·å¤§å­¦ | 2024.11 | - | NeurIPS 2024 | è½»é‡çº§VLAæ¨¡å‹ |
| TraceVLA | [paper](https://arxiv.org/pdf/2412.10345) | [code](https://github.com/umd-huang-lab/tracevla) | - | å¾®è½¯ | 2024.12 | - | ICLR 2025 | è½¨è¿¹è¿½è¸ªèƒ½åŠ› |

### 2. æ‰©æ•£æ¨¡å‹ (Diffusion Models)

| æ¨¡å‹åç§° | Paper | Code | Project/Blog | æœºæ„ | æ—¶é—´ | æ¨¡å‹å¤§å° | ä¼šè®®/æœŸåˆŠ | ç‰¹ç‚¹ |
|---------|-------|------|--------------|------|------|----------|-----------|------|
| Octo | [paper](https://arxiv.org/pdf/2405.12213) | [code](https://octo-models.github.io/) | - | æ–¯å¦ç¦ï¼Œä¼¯å…‹åˆ© | 2024.5 | 93M | RSS 2024 | è½»é‡çº§æ‰©æ•£æ¨¡å‹ |
| Ï€0 | [paper](https://arxiv.org/pdf/2410.24164) | [code](https://github.com/Physical-Intelligence/openpi) | - | æ–¯å¦ç¦, Physical Intelligence | 2024.10 | 3.3B | NeurIPS 2024 | åŸºäºæµçš„æ‰©æ•£VLAï¼Œä½¿ç”¨PaliGemma (3B VLM) |
| CogACT | [paper](https://arxiv.org/pdf/2411.19650) | [code](https://github.com/microsoft/CogACT.git) | - | æ¸…åï¼ŒMSRA | 2024.11 | 7B | ICLR 2025 | è®¤çŸ¥åŠ¨ä½œTransformer |
| Diffusion-VLA | [paper](https://arxiv.org/abs/2412.03293) | [code](https://arxiv.org/pdf/2410.07864) | - | åä¸œå¸ˆèŒƒï¼Œä¸Šæµ·å¤§å­¦ï¼Œç¾çš„ | 2024.12 | - | ICLR 2025 | æ‰©æ•£æ¨¡å‹VLA |

### 3. 3Dè§†è§‰æ¨¡å‹

| æ¨¡å‹åç§° | Paper | Code | Project/Blog | æœºæ„ | æ—¶é—´ | ä¼šè®®/æœŸåˆŠ | ç‰¹ç‚¹ |
|---------|-------|------|--------------|------|------|-----------|------|
| 3D-VLA | [paper](https://arxiv.org/pdf/2403.09631) | [code](https://github.com/UMass-Foundation-Model/3D-VLA/tree/main) | - | UMass | 2024.3 | CVPR 2024 | 3D-based LLM |
| SpatialVLA | [paper](https://arxiv.org/pdf/2501.15830) | [code](https://github.com/SpatialVLA/SpatialVLA) | - | ä¸Šæµ·AI Lab | 2025.1 | ICLR 2025 | Adaptive Action Grid |

### 4. äº§ä¸šçº§VLAæ¨¡å‹

| æ¨¡å‹åç§° | Paper | Code | Project/Blog | æœºæ„ | æ—¶é—´ | ä¼šè®®/æœŸåˆŠ | ç‰¹ç‚¹ |
|---------|-------|------|--------------|------|------|-----------|------|
| Figure: Helix | - | - | [Figure](https://www.figure.ai/news/helix) | Figure | 2025.2.20 | - | æœºå™¨äººå…¨èº«ä¸ŠåŠèº«æ§åˆ¶ |
| æ™ºå…ƒï¼šGO-1 | - | - | [æ™ºå…ƒå®˜ç½‘](https://www.zhiyuan-robot.com/article/189/detail/56.html) | æ™ºå…ƒ | 2025.3.10 | - | VLM+MoEæ¶æ„ |
| pi-0.5 | [paper](https://arxiv.org/abs/2504.16054) | [code](https://github.com/Physical-Intelligence/openpi) | [blog](https://blog.csdn.net/) | Physical Intelligence | 2025.4.22 | Science Robotics | å•æ¨¡å‹æ¶æ„ï¼Œé«˜çº§ä»»åŠ¡åˆ†è§£ |
| Hi Robot | [paper](https://arxiv.org/abs/2502.19417) | [code](https://github.com/Physical-Intelligence/openpi) | [blog](https://blog.csdn.net/) | Physical Intelligence | 2025.2.26 | Science Robotics | VLM+VLAåŒæ¨¡å‹æ¶æ„ |
| GROOT-N1 | [paper](https://arxiv.org/abs/2503.14734) | [code](https://github.com/NVIDIA/Isaac-GR00T) | [blog](https://developer.nvidia.com/blog/) | Nvidia | 2025.3.27 | Science Robotics | 2Bå‚æ•°ï¼Œå…¨èº«æ§åˆ¶ï¼ŒNVIDIA-Eagleæ¶æ„ |
| Psi-R1 | - | - | [blog](https://www.jiqizhixin.com/articles/2025-03-03-9) | çµåˆæ™ºèƒ½ | 2025.4.27 | - | åˆ†å±‚ç«¯åˆ°ç«¯VLA+RLï¼ŒéªŒè¯test-time scaling |
| Gemini Robotics | [paper](https://arxiv.org/pdf/2503.20020) | - | - | Google DeepMind | 2025.3.25 | Science Robotics | Gemini 2.0æ„å»ºï¼Œ50HzåŠ¨ä½œè¾“å‡ºé¢‘ç‡ |

### 5. æœ€æ–°VLAå·¥ä½œ

| æ¨¡å‹åç§° | Paper | Code | Project/Blog | æœºæ„ | æ—¶é—´ | æ¨¡å‹å¤§å° | ä¼šè®®/æœŸåˆŠ | ç‰¹ç‚¹ |
|---------|-------|------|--------------|------|------|----------|-----------|------|
| SafeVLA | [paper](https://arxiv.org/abs/2503.03480) | [code](https://github.com/PKU-Alignment/SafeVLA) | - | åŒ—å¤§ | 2025.3.5 | - | ICLR 2025 | è§£å†³ä¼ ç»ŸVLAæ¨¡å‹åœ¨æŠ“å–å’Œå¯¼èˆªä»»åŠ¡ä¸­çš„ä¸å®‰å…¨è¡Œä¸º |
| HybridVLA | [paper](https://arxiv.org/pdf/2503.10631) | [code](https://github.com/PKU-HMI-Lab/Hybrid-VLA) | - | åŒ—å¤§ | 2025.3.17 | 2.7B/7B | ICLR 2025 | ç”¨ç»Ÿä¸€æ¨¡å‹é›†æˆæ‰©æ•£å’Œè‡ªå›å½’åŠ¨ä½œé¢„æµ‹ |
| DexVLA | [paper](https://arxiv.org/pdf/2502.05855) | [code](https://github.com/juruobenruo/DexVLA) | - | ç¾çš„, ä¸œå—å¤§å­¦ | 2025.2.9 | 1B | ICLR 2025 | Diffusion expertï¼Œé‡‡ç”¨å¤šä¸ªaction head |
| DexGraspVLA | [paper](https://arxiv.org/abs/2502.20900) | [code](https://github.com/Psi-Robot/DexGraspVLA) | - | åŒ—å¤§ | 2025.2.28 | - | ICLR 2025 | çµå·§æ‰‹æŠ“å–VLA |
| UP-VLA | [paper](https://arxiv.org/pdf/2501.18867) | - | - | æ¸…å | 2025.2.3 | - | ICLR 2025 | åŠ å…¥goal-imageé¢„æµ‹ä»»åŠ¡å¸®åŠ©åŠ¨ä½œç”Ÿæˆ |
| CoT-VLA | [paper](https://arxiv.org/pdf/2503.22020) | - | - | Nvidia, Stanford | 2025.3 | 7B | CVPR 2025 | å°†CoTèå…¥VLAä¸­ï¼Œé€šè¿‡è‡ªå›å½’åœ°é¢„æµ‹æœªæ¥çš„å›¾åƒå¸§ä½œä¸ºè§†è§‰ç›®æ ‡ |
| UniAct | [paper](https://arxiv.org/abs/2501.10105) | [code](https://github.com/2toinf/UniAct) | - | æ¸…å | 2025.1 | - | CVPR 2025 | åŸºäºé€šç”¨åŠ¨ä½œç©ºé—´çš„å…·èº«åŸºç¡€æ¨¡å‹ |
| UniVLA | [paper](https://arxiv.org/pdf/2505.06111) | [code](https://github.com/OpenDriveLab/UniVLA) | - | æ¸¯å¤§, ä¸Šæµ·AI Lab, æ™ºå…ƒ | 2025.5.9 | - | ICLR 2025 | åˆ©ç”¨æ½œåœ¨åŠ¨ä½œæ¨¡å‹ä»å¤šæ ·åŒ–æ•°æ®ä¸­æå–ä»»åŠ¡æ ¸å¿ƒè¡¨å¾ï¼Œæå‡æ³›åŒ–èƒ½åŠ› |


<section id="acknowledgement"></section>

# Acknowledgement
æœ¬æ–‡å‚è€ƒ/å¼•ç”¨äº†ä¸€äº›åšä¸»çš„æ–‡ç« , æˆ‘ä»¬å¯¹ä»–ä»¬çš„çŸ¥è¯†åˆ†äº«è¡¨ç¤ºæ„Ÿè°¢, å¼•ç”¨åˆ—è¡¨å¦‚ä¸‹ï¼š
[1] Github [Tianxing Chen](https://github.com/TianxingChen/Embodied-AI-Guide)

<section id="cite"></section>

# ğŸ‘ Citation
If you find this repository helpful, please consider citing:

```
@misc{vlaresearch2025,
      title = {VLA-Research},
      author = {VLA-Research-Contributors},
      year = {2025},
      howpublished = {\url{https://github.com/Louis-ZhangLe/VLA-Research}},
}
```

<section id="license"></section>

# ğŸ·ï¸ License
This repository is released under the MIT license. See [LICENSE](./LICENSE) for additional details.
