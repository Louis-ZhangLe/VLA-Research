<h1 align="center">VLA-Research</h1>

<p align="center"> </p>


> VLA-Research includes papers and open-source projects related to VLA, especially in the field of embodied manipulation. It aims to collect and summarize all VLA work, hoping to help friends with embodied AI to understand VLA work efficiently. Welcome to click Star, share and communicate.
# ğŸ“ TODO
| ä»»åŠ¡ | çŠ¶æ€ | æè¿° |
|------|------|------|
| ğŸ”µ VLAåˆ†ç±» | å¾…å®Œæˆ | å¯¹VLAæ¨¡å‹è¿›è¡Œåˆ†ç±»æ•´ç† |
| ğŸ’  VLA-RLæ•´ç† | å¾…å®Œæˆ | æ•´ç†VLAä¸å¼ºåŒ–å­¦ä¹ ç›¸å…³çš„å·¥ä½œ |

# Contents

<nav>
  <ul>
    <li><a href="#VLA Research List">VLA Research List</a></li>
    <li><a href="#acknowledgement">Acknowledgement</a></li>
    <li><a href="#cite">ğŸ‘ Citation</a></li>
    <li><a href="#license">ğŸ·ï¸ License</a></li>
  </ul>
</nav>


# VLA Research List

<section id="VLA Research List"></section>



| æ¨¡å‹åç§° | Paper | Code | Project | æœºæ„ | æ—¶é—´ | æ¨¡å‹å¤§å° | ä¼šè®®/æœŸåˆŠ | ç‰¹ç‚¹ |
|---------|-------|------|--------------|------|------|----------|-----------|------|
| RT-1 | [paper](https://arxiv.org/pdf/2212.06817) | [code](https://github.com/google-research/robotics_transformer) | [page](https://robotics-transformer1.github.io/) | Google Deepmind | 2022.12 | - | - | æœºå™¨äººTransformerç³»åˆ—çš„å¼€å±±ä¹‹ä½œ |
| RT-2 | [paper](https://arxiv.org/pdf/2307.15818) | - | [page](https://robotics-transformer2.github.io/) | Google Deepmind | 2023.7 | 55B |  | æœºå™¨äººTransformerç¬¬äºŒä»£ï¼Œæ”¯æŒè§†è§‰æ¨ç† |
| RT-Trajectory | [paper](https://arxiv.org/pdf/2311.01977) | - | [page](https://rt-trajectory.github.io/) | Google Deepmind, UCSD, æ–¯å¦ç¦ | 2023.11 | - | ICLR 2024 (Spotlight) | è½¨è¿¹é¢„æµ‹èƒ½åŠ›å¢å¼º |
| AUTO-RT | [paper](https://arxiv.org/pdf/2401.12963) | - | [page](https://auto-rt.github.io/) | Google Deepmind | 2024.1 | - |  | è‡ªåŠ¨åŒ–æœºå™¨äººTransformer |
| RoboFlamingo | [paper](https://arxiv.org/pdf/2311.01378) | [code](https://github.com/roboflamingo) | - | å­—èŠ‚ã€æ¸…å | 2024.2 | - |  | åŸºäºFlamingoæ¶æ„ |
| OpenVLA | [paper](https://arxiv.org/pdf/2406.09246) | [code](https://github.com/openvla) | - | Stanford | 2024.6 | 7B |  | å¼€æºVLAæ¨¡å‹ |
| TinyVLA | [paper](https://arxiv.org/abs/2409.12514) | - | - | ä¸Šæµ·å¤§å­¦ | 2024.11 | - | | è½»é‡çº§VLAæ¨¡å‹ |
| TraceVLA | [paper](https://arxiv.org/pdf/2412.10345) | [code](https://github.com/umd-huang-lab/tracevla) | - | å¾®è½¯ | 2024.12 | - |  | è½¨è¿¹è¿½è¸ªèƒ½åŠ› |
| Octo | [paper](https://arxiv.org/pdf/2405.12213) | [code](https://octo-models.github.io/) | - | æ–¯å¦ç¦ï¼Œä¼¯å…‹åˆ© | 2024.5 | 93M |  | è½»é‡çº§æ‰©æ•£æ¨¡å‹ |
| Ï€0 | [paper](https://arxiv.org/pdf/2410.24164) | [code](https://github.com/Physical-Intelligence/openpi) | - | æ–¯å¦ç¦, Physical Intelligence | 2024.10 | 3.3B |  | åŸºäºæµçš„æ‰©æ•£VLAï¼Œä½¿ç”¨PaliGemma (3B VLM) |
| CogACT | [paper](https://arxiv.org/pdf/2411.19650) | [code](https://github.com/microsoft/CogACT.git) | - | æ¸…åï¼ŒMSRA | 2024.11 | 7B |  | è®¤çŸ¥åŠ¨ä½œTransformer |
| Diffusion-VLA | [paper](https://arxiv.org/abs/2412.03293) | [code](https://arxiv.org/pdf/2410.07864) | - | åä¸œå¸ˆèŒƒï¼Œä¸Šæµ·å¤§å­¦ï¼Œç¾çš„ | 2024.12 | - |  | æ‰©æ•£æ¨¡å‹VLA |
| 3D-VLA | [paper](https://arxiv.org/pdf/2403.09631) | [code](https://github.com/UMass-Foundation-Model/3D-VLA/tree/main) | - | UMass | 2024.3 | CVPR 2024 | 3D-based LLM |
| SpatialVLA | [paper](https://arxiv.org/pdf/2501.15830) | [code](https://github.com/SpatialVLA/SpatialVLA) | - | ä¸Šæµ·AI Lab | 2025.1 |  | Adaptive Action Grid |
| Figure: Helix | - | - | [Figure](https://www.figure.ai/news/helix) | Figure | 2025.2.20 | - | æœºå™¨äººå…¨èº«ä¸ŠåŠèº«æ§åˆ¶ |
| GO-1 | - | - | [æ™ºå…ƒå®˜ç½‘](https://www.zhiyuan-robot.com/article/189/detail/56.html) | æ™ºå…ƒ | 2025.3.10 | - | VLM+MoEæ¶æ„ |
| pi-0.5 | [paper](https://arxiv.org/abs/2504.16054) | [code](https://github.com/Physical-Intelligence/openpi) | [blog](https://blog.csdn.net/) | Physical Intelligence | 2025.4.22 | | å•æ¨¡å‹æ¶æ„ï¼Œé«˜çº§ä»»åŠ¡åˆ†è§£ |
| Hi Robot | [paper](https://arxiv.org/abs/2502.19417) | [code](https://github.com/Physical-Intelligence/openpi) | [blog](https://blog.csdn.net/) | Physical Intelligence | 2025.2.26 | | VLM+VLAåŒæ¨¡å‹æ¶æ„ |
| GROOT-N1 | [paper](https://arxiv.org/abs/2503.14734) | [code](https://github.com/NVIDIA/Isaac-GR00T) | [blog](https://developer.nvidia.com/blog/) | Nvidia | 2025.3.27 |  | 2Bå‚æ•°ï¼Œå…¨èº«æ§åˆ¶ï¼ŒNVIDIA-Eagleæ¶æ„ |
| Psi-R1 | - | - | [blog](https://www.jiqizhixin.com/articles/2025-03-03-9) | çµåˆæ™ºèƒ½ | 2025.4.27 | - | åˆ†å±‚ç«¯åˆ°ç«¯VLA+RLï¼ŒéªŒè¯test-time scaling |
| Gemini Robotics | [paper](https://arxiv.org/pdf/2503.20020) | - | - | Google DeepMind | 2025.3.25 |  | Gemini 2.0æ„å»ºï¼Œ50HzåŠ¨ä½œè¾“å‡ºé¢‘ç‡ |
| SafeVLA | [paper](https://arxiv.org/abs/2503.03480) | [code](https://github.com/PKU-Alignment/SafeVLA) | - | åŒ—å¤§ | 2025.3.5 | - |  | è§£å†³ä¼ ç»ŸVLAæ¨¡å‹åœ¨æŠ“å–å’Œå¯¼èˆªä»»åŠ¡ä¸­çš„ä¸å®‰å…¨è¡Œä¸º |
| HybridVLA | [paper](https://arxiv.org/pdf/2503.10631) | [code](https://github.com/PKU-HMI-Lab/Hybrid-VLA) | - | åŒ—å¤§ | 2025.3.17 | 2.7B/7B |  | ç”¨ç»Ÿä¸€æ¨¡å‹é›†æˆæ‰©æ•£å’Œè‡ªå›å½’åŠ¨ä½œé¢„æµ‹ |
| DexVLA | [paper](https://arxiv.org/pdf/2502.05855) | [code](https://github.com/juruobenruo/DexVLA) | - | ç¾çš„, ä¸œå—å¤§å­¦ | 2025.2.9 | 1B |  | Diffusion expertï¼Œé‡‡ç”¨å¤šä¸ªaction head |
| DexGraspVLA | [paper](https://arxiv.org/abs/2502.20900) | [code](https://github.com/Psi-Robot/DexGraspVLA) | - | åŒ—å¤§ | 2025.2.28 | - |  | çµå·§æ‰‹æŠ“å–VLA |
| UP-VLA | [paper](https://arxiv.org/pdf/2501.18867) | - | - | æ¸…å | 2025.2.3 | - |  | åŠ å…¥goal-imageé¢„æµ‹ä»»åŠ¡å¸®åŠ©åŠ¨ä½œç”Ÿæˆ |
| CoT-VLA | [paper](https://arxiv.org/pdf/2503.22020) | - | - | Nvidia, Stanford | 2025.3 | 7B |  | å°†CoTèå…¥VLAä¸­ï¼Œé€šè¿‡è‡ªå›å½’åœ°é¢„æµ‹æœªæ¥çš„å›¾åƒå¸§ä½œä¸ºè§†è§‰ç›®æ ‡ |
| UniAct | [paper](https://arxiv.org/abs/2501.10105) | [code](https://github.com/2toinf/UniAct) | - | æ¸…å | 2025.1 | - |  | åŸºäºé€šç”¨åŠ¨ä½œç©ºé—´çš„å…·èº«åŸºç¡€æ¨¡å‹ |
| UniVLA | [paper](https://arxiv.org/pdf/2505.06111) | [code](https://github.com/OpenDriveLab/UniVLA) | - | æ¸¯å¤§, ä¸Šæµ·AI Lab, æ™ºå…ƒ | 2025.5.9 | - |  | åˆ©ç”¨æ½œåœ¨åŠ¨ä½œæ¨¡å‹ä»å¤šæ ·åŒ–æ•°æ®ä¸­æå–ä»»åŠ¡æ ¸å¿ƒè¡¨å¾ï¼Œæå‡æ³›åŒ–èƒ½åŠ› |


<section id="acknowledgement"></section>

# Acknowledgement
æœ¬æ–‡å‚è€ƒ/å¼•ç”¨äº†ä¸€äº›åšä¸»çš„æ–‡ç« , æˆ‘ä»¬å¯¹ä»–ä»¬çš„çŸ¥è¯†åˆ†äº«è¡¨ç¤ºæ„Ÿè°¢, å¼•ç”¨åˆ—è¡¨å¦‚ä¸‹ï¼š[1] çŸ¥ä¹ [å¼ ä¹](https://www.zhihu.com/people/zhang-le-3-67-28)
[1] Github [Tianxing Chen](https://github.com/TianxingChen/Embodied-AI-Guide)

<section id="cite"></section>

# ğŸ‘ Citation
If you find this repository helpful, please consider citing:

```
@misc{vlaresearch2025,
      title = {VLA-Research},
      author = {VLA-Research-Contributors},
      year = {2025},
      howpublished = {\url{https://github.com/Louis-ZhangLe/VLA-Research}},
}
```

<section id="license"></section>

# ğŸ·ï¸ License
This repository is released under the MIT license. See [LICENSE](./LICENSE) for additional details.
